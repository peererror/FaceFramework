{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python train_mask_detector.py --dataset dataset\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"dataset\":\"dataset\",\"plot\":\"plot.png\",\"model\":\"mask_detector_v4.h5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:952: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 16:23:39.771328 23740 mobilenet_v2.py:280] `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "#     help=\"path to input dataset\")\n",
    "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "#     help=\"path to output loss/accuracy plot\")\n",
    "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "#     default=\"mask_detector.model\",\n",
    "#     help=\"path to output face mask detector model\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32\n",
    "\n",
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    # load the input image (224x224) and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.20, stratify=labels, random_state=42)\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
    "# left off\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = data\n",
    "trainY = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Epoch 1/30\n",
      "193/193 [==============================] - 117s 608ms/step - loss: 0.3851 - accuracy: 0.8305\n",
      "Epoch 2/30\n",
      "193/193 [==============================] - 118s 614ms/step - loss: 0.1972 - accuracy: 0.9203\n",
      "Epoch 3/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.1718 - accuracy: 0.9301\n",
      "Epoch 4/30\n",
      "193/193 [==============================] - 118s 609ms/step - loss: 0.1614 - accuracy: 0.9334\n",
      "Epoch 5/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.1523 - accuracy: 0.9400\n",
      "Epoch 6/30\n",
      "193/193 [==============================] - 118s 610ms/step - loss: 0.1386 - accuracy: 0.9475\n",
      "Epoch 7/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1318 - accuracy: 0.9489\n",
      "Epoch 8/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1240 - accuracy: 0.9533\n",
      "Epoch 9/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1241 - accuracy: 0.9541\n",
      "Epoch 10/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.1209 - accuracy: 0.9524\n",
      "Epoch 11/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1137 - accuracy: 0.9580\n",
      "Epoch 12/30\n",
      "193/193 [==============================] - 117s 608ms/step - loss: 0.1082 - accuracy: 0.9605\n",
      "Epoch 13/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1061 - accuracy: 0.9605\n",
      "Epoch 14/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.1108 - accuracy: 0.9561\n",
      "Epoch 15/30\n",
      "193/193 [==============================] - 117s 609ms/step - loss: 0.1016 - accuracy: 0.9603\n",
      "Epoch 16/30\n",
      "193/193 [==============================] - 117s 608ms/step - loss: 0.0983 - accuracy: 0.9621\n",
      "Epoch 17/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.0967 - accuracy: 0.9648\n",
      "Epoch 18/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.0954 - accuracy: 0.9645\n",
      "Epoch 19/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.0957 - accuracy: 0.9619\n",
      "Epoch 20/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.0968 - accuracy: 0.9614\n",
      "Epoch 21/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.0919 - accuracy: 0.9677\n",
      "Epoch 22/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.0932 - accuracy: 0.9658\n",
      "Epoch 23/30\n",
      "193/193 [==============================] - 117s 607ms/step - loss: 0.0913 - accuracy: 0.9648\n",
      "Epoch 24/30\n",
      "193/193 [==============================] - 117s 608ms/step - loss: 0.0903 - accuracy: 0.9650\n",
      "Epoch 25/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.0834 - accuracy: 0.9669\n",
      "Epoch 26/30\n",
      "193/193 [==============================] - 118s 612ms/step - loss: 0.0840 - accuracy: 0.9681\n",
      "Epoch 27/30\n",
      "193/193 [==============================] - 117s 604ms/step - loss: 0.0805 - accuracy: 0.9684\n",
      "Epoch 28/30\n",
      "193/193 [==============================] - 117s 606ms/step - loss: 0.0779 - accuracy: 0.9699\n",
      "Epoch 29/30\n",
      "193/193 [==============================] - 117s 608ms/step - loss: 0.0786 - accuracy: 0.9684\n",
      "Epoch 30/30\n",
      "193/193 [==============================] - 116s 604ms/step - loss: 0.0778 - accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "  #  validation_data=(testX, testY),\n",
    "   # validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "    target_names=lb.classes_))\n",
    "\n",
    "\n",
    "print(\"[INFO] saving mask detector model...\")\n",
    "model.save(args[\"model\"], save_format=\"h5\")\n",
    "\n",
    "\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(args[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"model\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
