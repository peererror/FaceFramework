{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "from matplotlib import pyplot as plt\n",
    "import imutils\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "class Landmark_Extractor:\n",
    "\n",
    "    @staticmethod\n",
    "    def init_plugin(device,cpu_extension_path = \"\"):\n",
    "        plugin = IEPlugin(device = device)\n",
    "        if device == \"CPU\" and cpu_extension_path!=\"\":\n",
    "            plugin.add_cpu_extension(cpu_extension_path)\n",
    "        return plugin\n",
    "    \n",
    "    def load_net(self,model_xml,model_bin,plugin,num_requests = 2):\n",
    "        net = IENetwork(model = model_xml, weights= model_bin)\n",
    "        not_supported_layers = []\n",
    "\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        net_layers = net.layers\n",
    "\n",
    "        for layer in supported_layers:\n",
    "            if not layer in supported_layers:\n",
    "                not_supported_layers.append(layer)\n",
    "        \n",
    "        if len(not_supported_layers)>0:\n",
    "            print(\"WARNING: None supported layers detected, please review network artchtecture before continuing...\")\n",
    "            print(not_supported_layers)\n",
    "        else:\n",
    "            print(\"INFO: All network layers are supported.\")\n",
    "        \n",
    "        self.exec_net = plugin.load(network=net,num_requests=2)\n",
    "    \n",
    "        self.input_blob = next(iter(net.inputs))\n",
    "        self.output_blob = next(iter(net.outputs))\n",
    "\n",
    "        self.input_shape = net.inputs[self.input_blob].shape\n",
    "        self.output_shape = net.outputs[self.output_blob].shape\n",
    "\n",
    "        print(\"Input Shape: {}\".format(self.input_shape))\n",
    "        print(\"Output Shape: {}\".format(self.output_shape))\n",
    "    \n",
    "    \n",
    "    def extractLandmarks(self,img):\n",
    "        img = cv2.resize(img,(self.input_shape[2],self.input_shape[3]))\n",
    "        img = img.transpose((2, 0, 1))  \n",
    "        img = np.expand_dims(img,0)\n",
    "    \n",
    "        pred = self.exec_net.infer(inputs = {self.input_blob:img})\n",
    "        landmarks = pred[self.output_blob][0]\n",
    "        landmarks_pairs = []\n",
    "        for i in range(0,len(landmarks),2):\n",
    "            landmarks_pairs.append((landmarks[i],landmarks[i+1]))\n",
    "        return landmarks_pairs\n",
    "    \n",
    "    def calculate_reference_landmarks(self,img):\n",
    "        reference_landmarks = [(0.31556875000000000*img.shape[1], 0.4615741071428571*img.shape[0]),\n",
    " (0.68262291666666670*img.shape[1], 0.4615741071428571*img.shape[0]),\n",
    " (0.50026249999999990*img.shape[1], 0.6405053571428571*img.shape[0]),\n",
    " (0.34947187500000004*img.shape[1], 0.8246919642857142*img.shape[0]),\n",
    " (0.65343645833333330*img.shape[1], 0.8246919642857142*img.shape[0])]\n",
    "        return reference_landmarks\n",
    "    \n",
    "    def process_landmarks(self,landmarks,img):\n",
    "        left_eye_points = tuple(np.mean((landmarks[0],landmarks[1]),axis=0))\n",
    "        right_eye_points =tuple(np.mean((landmarks[2],landmarks[3]),axis=0))\n",
    "        nose_tip_points =  landmarks[4]\n",
    "        left_lip_corner_points = landmarks[8]\n",
    "        right_lip_corner_points = landmarks[9]\n",
    "\n",
    "        extracted_landmarks = [left_eye_points,right_eye_points,nose_tip_points,left_lip_corner_points\n",
    "                               ,right_lip_corner_points]\n",
    "        extracted_landmarks = list(map(lambda x:(x[0]*img.shape[1],x[1]*img.shape[0]),extracted_landmarks))\n",
    "        return extracted_landmarks\n",
    "    \n",
    "    \n",
    "    def procrustes2(self,X, Y, scaling=True, reflection='best'):\n",
    "        \"\"\"\n",
    "    A port of MATLAB's `procrustes` function to Numpy.\n",
    "\n",
    "    Procrustes analysis determines a linear transformation (translation,\n",
    "    reflection, orthogonal rotation and scaling) of the points in Y to best\n",
    "    conform them to the points in matrix X, using the sum of squared errors\n",
    "    as the goodness of fit criterion.\n",
    "\n",
    "        d, Z, [tform] = procrustes(X, Y)\n",
    "\n",
    "    Inputs:\n",
    "    ------------\n",
    "    X, Y    \n",
    "        matrices of target and input coordinates. they must have equal\n",
    "        numbers of  points (rows), but Y may have fewer dimensions\n",
    "        (columns) than X.\n",
    "\n",
    "    scaling \n",
    "        if False, the scaling component of the transformation is forced\n",
    "        to 1\n",
    "\n",
    "    reflection\n",
    "        if 'best' (default), the transformation solution may or may not\n",
    "        include a reflection component, depending on which fits the data\n",
    "        best. setting reflection to True or False forces a solution with\n",
    "        reflection or no reflection respectively.\n",
    "\n",
    "    Outputs\n",
    "    ------------\n",
    "    d       \n",
    "        the residual sum of squared errors, normalized according to a\n",
    "        measure of the scale of X, ((X - X.mean(0))**2).sum()\n",
    "\n",
    "    Z\n",
    "        the matrix of transformed Y-values\n",
    "\n",
    "    tform   \n",
    "        a dict specifying the rotation, translation and scaling that\n",
    "        maps X --> Y\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n,m = X.shape\n",
    "        ny,my = Y.shape\n",
    "\n",
    "        muX = X.mean(0)\n",
    "        muY = Y.mean(0)\n",
    "\n",
    "        X0 = X - muX\n",
    "        Y0 = Y - muY\n",
    "\n",
    "        ssX = (X0**2.).sum()\n",
    "        ssY = (Y0**2.).sum()\n",
    "\n",
    "    # centred Frobenius norm\n",
    "        normX = np.sqrt(ssX)\n",
    "        normY = np.sqrt(ssY)\n",
    "\n",
    "    # scale to equal (unit) norm\n",
    "        X0 /= normX\n",
    "        Y0 /= normY\n",
    "\n",
    "        if my < m:\n",
    "            Y0 = np.concatenate((Y0, np.zeros(n, m-my)),0)\n",
    "\n",
    "    # optimum rotation matrix of Y\n",
    "        A = np.dot(X0.T, Y0)\n",
    "        U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "        V = Vt.T\n",
    "        T = np.dot(V, U.T)\n",
    "\n",
    "        if reflection is not 'best':\n",
    "\n",
    "        # does the current solution use a reflection?\n",
    "            have_reflection = np.linalg.det(T) < 0\n",
    "\n",
    "        # if that's not what was specified, force another reflection\n",
    "            if reflection != have_reflection:\n",
    "                V[:,-1] *= -1\n",
    "                s[-1] *= -1\n",
    "                T = np.dot(V, U.T)\n",
    "\n",
    "        traceTA = s.sum()\n",
    "\n",
    "        if scaling:\n",
    "\n",
    "        # optimum scaling of Y\n",
    "            b = traceTA * normX / normY\n",
    "\n",
    "        # standarised distance between X and b*Y*T + c\n",
    "            d = 1 - traceTA**2\n",
    "\n",
    "        # transformed coords\n",
    "            Z = normX*traceTA*np.dot(Y0, T) + muX\n",
    "\n",
    "        else:\n",
    "            b = 1\n",
    "            d = 1 + ssY/ssX - 2 * traceTA * normY / normX\n",
    "            Z = normY*np.dot(Y0, T) + muX\n",
    "\n",
    "    # transformation matrix\n",
    "        if my < m:\n",
    "            T = T[:my,:]\n",
    "        c = muX - b*np.dot(muY, T)\n",
    "\n",
    "    #transformation values \n",
    "        tform = {'rotation':T, 'scale':b, 'translation':c}\n",
    "\n",
    "        return d, Z, tform\n",
    "    \n",
    "    \n",
    "    def transform_face(self,img,landmarks,reference_landmarks,visualize = False):\n",
    "\n",
    "        extracted_landmarks = self.process_landmarks(landmarks,img)\n",
    "        d,Z_pts,Tform = self.procrustes2(np.asarray(reference_landmarks),np.asarray(extracted_landmarks))\n",
    "\n",
    "        R = np.eye(3)\n",
    "        R[0:2,0:2] = Tform['rotation']\n",
    "        S = np.eye(3) * Tform['scale'] \n",
    "        S[2,2] = 1\n",
    "        t = np.eye(3)\n",
    "        t[0:2,2] = Tform['translation']\n",
    "        transform_mat = np.dot(np.dot(R,S),t.T).T\n",
    "\n",
    "        img = cv2.warpAffine(img,transform_mat[0:2,:],(img.shape[1],img.shape[0]))\n",
    "    \n",
    "        if visualize:\n",
    "            img_vis = img.copy()\n",
    "            for (x,y) in reference_landmarks:\n",
    "                x = int(x)\n",
    "                y = int(y)\n",
    "                cv2.circle(img_vis,(x,y),int(img_vis.shape[0]/30),(0,255,0),-1)\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(img_vis,cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def prepare_face(self,img,visualize = False,verbose=False):\n",
    "        if verbose:\n",
    "            start = time.time()\n",
    "            landmarks = self.extractLandmarks(img)\n",
    "            end = time.time()\n",
    "            print(\"Landmarks extracted in {} milliesconds.\".format(int((end-start)*1000)))\n",
    "            start = time.time()\n",
    "            T = self.transform_face(img,landmarks,self.calculate_reference_landmarks(img),visualize)\n",
    "            end = time.time()\n",
    "            print(\"Face re-alligned in {} milliesconds.\".format(int((end-start)*1000)))\n",
    "        else:\n",
    "            landmarks = self.extractLandmarks(img)\n",
    "            T = self.transform_face(img,landmarks,self.calculate_reference_landmarks(img),visualize)\n",
    "        return T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openvino.inference_engine.ie_api.IEPlugin at 0x1f2aec6ea08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Landmark_Extractor.init_plugin(\"MYRIAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Python modules.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[275]:\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "from matplotlib import pyplot as plt\n",
    "import imutils\n",
    "print(\"Imported Python modules.\")\n",
    "\n",
    "\n",
    "# In[278]:\n",
    "\n",
    "\n",
    "class Face_Detector:\n",
    "    def load_net(self,model_xml,model_bin,device,cpu_extension_path):\n",
    "        plugin = IEPlugin(device = device)\n",
    "        plugin.set_config({\"VPU_FORCE_RESET\": \"NO\"})\n",
    "        #plugin.set_config({\"PERF_COUNT\":\"YES\"})\n",
    "        if device == \"CPU\" and cpu_extension_path!=\"\":\n",
    "            plugin.add_cpu_extension(cpu_extension_path)\n",
    "        net = IENetwork(model = model_xml, weights= model_bin)\n",
    "        not_supported_layers = []\n",
    "\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        net_layers = net.layers\n",
    "\n",
    "        for layer in supported_layers:\n",
    "            if not layer in supported_layers:\n",
    "                not_supported_layers.append(layer)\n",
    "        \n",
    "        if len(not_supported_layers)>0:\n",
    "            print(\"WARNING: None supported layers detected, please review network artchtecture before continuing...\")\n",
    "            print(not_supported_layers)\n",
    "        else:\n",
    "            print(\"INFO: All network layers are supported.\")\n",
    "        \n",
    "        self.exec_net = plugin.load(network=net,num_requests=10)\n",
    "    \n",
    "        self.input_blob = next(iter(net.inputs))\n",
    "        self.output_blob = next(iter(net.outputs))\n",
    "\n",
    "        self.input_shape = net.inputs[self.input_blob].shape\n",
    "        self.output_shape = net.outputs[self.output_blob].shape\n",
    "\n",
    "        print(\"Input Shape: {}\".format(self.input_shape))\n",
    "        print(\"Output Shape: {}\".format(self.output_shape))\n",
    "        \n",
    "    \n",
    "    def detectFaces(self,img,face_thresh = 0.7):\n",
    "        orig_w = img.shape[1]\n",
    "        orig_h = img.shape[0]\n",
    "        img = cv2.resize(img,(self.input_shape[2],self.input_shape[3]))\n",
    "        img = img.transpose((2, 0, 1))  \n",
    "        img = np.expand_dims(img,0)\n",
    "\n",
    "\n",
    "        pred = self.exec_net.infer(inputs = {self.input_blob:img})\n",
    "        faces_detected = 0\n",
    "        face_coords = []\n",
    "\n",
    "        for face in pred[self.output_blob][0]:\n",
    "            for (image_id, label, conf, x_min, y_min, x_max, y_max) in face:\n",
    "                if(label == 1 and conf>face_thresh):\n",
    "                    faces_detected+=1\n",
    "                    face_coords.append((int(x_min*orig_w), int(y_min*orig_h)\n",
    "                                    , int(x_max*orig_w), int(y_max*orig_h)))\n",
    "\n",
    "        return face_coords,faces_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Python modules.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "from matplotlib import pyplot as plt\n",
    "import imutils\n",
    "print(\"Imported Python modules.\")\n",
    "\n",
    "\n",
    "\n",
    "plugin = IEPlugin(device = \"MYRIAD\")\n",
    "#plugin.set_config({\"VPU_FORCE_RESET\": \"NO\"})\n",
    "#plugin.set_config({\"PERF_COUNT\":\"YES\"})\n",
    "\n",
    "model_xml = r\"Models\\openvino\\face-detection\\FP16\\face-detection-retail-0004.xml\"\n",
    "model_bin = r\"Models\\openvino\\face-detection\\FP16\\face-detection-retail-0004.bin\"\n",
    "\n",
    "net1 = IENetwork(model = model_xml, weights= model_bin)\n",
    "exec_net1 = plugin.load(network=net1,num_requests=2)\n",
    "\n",
    "\n",
    "model_xml = r\"Models\\openvino\\gaze-estimation\\FP16\\head-pose-estimation-adas-0001.xml\"\n",
    "model_bin =r\"Models\\openvino\\gaze-estimation\\FP16\\head-pose-estimation-adas-0001.bin\"\n",
    "\n",
    "net2 = IENetwork(model = model_xml, weights= model_bin)\n",
    "exec_net2 = plugin.load(network=net2,num_requests=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_out': array([[[[0.        , 1.        , 1.        , ..., 0.06390381,\n",
       "           0.14733887, 0.30395508],\n",
       "          [0.        , 1.        , 1.        , ..., 0.06243896,\n",
       "           0.30029297, 0.30004883],\n",
       "          [0.        , 1.        , 1.        , ..., 0.06921387,\n",
       "           0.47021484, 0.29833984],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]]]], dtype=float32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\Images\\query\\Mind Grenades - Fake Faces 2.jpg\")\n",
    "\n",
    "input_blob = next(iter(net1.inputs))\n",
    "output_blob = next(iter(net1.outputs))\n",
    "\n",
    "input_shape = net1.inputs[input_blob].shape\n",
    "output_shape = net1.outputs[output_blob].shape\n",
    "\n",
    "img = cv2.resize(img,(input_shape[2],input_shape[3]))\n",
    "img = img.transpose((2, 0, 1)) \n",
    "img = np.expand_dims(img,0)\n",
    "\n",
    "#pred = exec_net1.infer(inputs = {input_blob:img})\n",
    "#pred\n",
    "\n",
    "exec_net1.start_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angle_p_fc': array([[-6.5273438]], dtype=float32),\n",
       " 'angle_r_fc': array([[-4.1523438]], dtype=float32),\n",
       " 'angle_y_fc': array([[-2.5585938]], dtype=float32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\Images\\query\\Mind Grenades - Fake Faces 2.jpg\")\n",
    "\n",
    "input_blob = next(iter(net2.inputs))\n",
    "output_blob = next(iter(net2.outputs))\n",
    "\n",
    "input_shape = net2.inputs[input_blob].shape\n",
    "output_shape = net2.outputs[output_blob].shape\n",
    "\n",
    "img = cv2.resize(img,(input_shape[2],input_shape[3]))\n",
    "img = img.transpose((2, 0, 1))  \n",
    "img = np.expand_dims(img,0)\n",
    "\n",
    "pred = exec_net2.infer(inputs = {input_blob:img})\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GazeEstimator:\n",
    "    def load_net(self,model_xml,model_bin,device,cpu_extension_path):\n",
    "        plugin = IEPlugin(device = device)\n",
    "        plugin.set_config({\"VPU_FORCE_RESET\": \"NO\"})\n",
    "        #plugin.set_config({\"PERF_COUNT\":\"YES\"})\n",
    "        if device == \"CPU\" and cpu_extension_path!=\"\":\n",
    "            plugin.add_cpu_extension(cpu_extension_path)\n",
    "        net = IENetwork(model = model_xml, weights= model_bin)\n",
    "        not_supported_layers = []\n",
    "\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        net_layers = net.layers\n",
    "\n",
    "        for layer in supported_layers:\n",
    "            if not layer in supported_layers:\n",
    "                not_supported_layers.append(layer)\n",
    "        \n",
    "        if len(not_supported_layers)>0:\n",
    "            print(\"WARNING: None supported layers detected, please review network artchtecture before continuing...\")\n",
    "            print(not_supported_layers)\n",
    "        else:\n",
    "            print(\"INFO: All network layers are supported.\")\n",
    "        \n",
    "        self.exec_net = plugin.load(network=net,num_requests=2)\n",
    "    \n",
    "        self.input_blob = next(iter(net.inputs))\n",
    "        self.output_blob_y = \"angle_y_fc\"\n",
    "        self.output_blob_p = \"angle_p_fc\"\n",
    "        self.output_blob_r = \"angle_r_fc\"\n",
    "\n",
    "        self.input_shape = net.inputs[self.input_blob].shape\n",
    "        self.output_shape = net.outputs[self.output_blob_y].shape\n",
    "\n",
    "        print(\"Input Shape: {}\".format(self.input_shape))\n",
    "        print(\"Output Shape: {}\".format(self.output_shape) + \"*3 for (Y,P,R)\")\n",
    "        \n",
    "    \n",
    "    def detectFaces(self,img):\n",
    "        img = cv2.resize(img,(self.input_shape[2],self.input_shape[3]))\n",
    "        img = img.transpose((2, 0, 1))  \n",
    "        img = np.expand_dims(img,0)\n",
    "\n",
    "        pred = self.exec_net.infer(inputs = {self.input_blob:img})\n",
    "        \n",
    "        return pred[self.output_blob_y][0][0],pred[self.output_blob_p][0][0],pred[self.output_blob_r][0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 300, 300]\n",
      "Output Shape: [1, 1, 200, 7]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models\\openvino\\face-detection\\FP16\\face-detection-retail-0004.xml\"\n",
    "model_bin = r\"Models\\openvino\\face-detection\\FP16\\face-detection-retail-0004.bin\"\n",
    "\n",
    "detector = Face_Detector()\n",
    "\n",
    "detector.load_net(model_xml,model_bin,\"MYRIAD\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can not init Myriad device: NC_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-471db2cc354b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGazeEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_xml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_bin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"MYRIAD\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cpu_extension_avx2.dll\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-03e55bf09042>\u001b[0m in \u001b[0;36mload_net\u001b[1;34m(self, model_xml, model_bin, device, cpu_extension_path)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"INFO: All network layers are supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexec_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_requests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_blob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IEPlugin.load\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IEPlugin.load\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can not init Myriad device: NC_ERROR"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models\\openvino\\gaze-estimation\\FP16\\head-pose-estimation-adas-0001.xml\"\n",
    "model_bin =r\"Models\\openvino\\gaze-estimation\\FP16\\head-pose-estimation-adas-0001.bin\"\n",
    "\n",
    "estimator = GazeEstimator()\n",
    "\n",
    "estimator.load_net(model_xml,model_bin,\"MYRIAD\",\"cpu_extension_avx2.dll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plugin = IEPlugin(device = \"MYRIAD\")\n",
    "plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 300, 300]\n",
      "Output Shape: [1, 1, 200, 7]\n"
     ]
    }
   ],
   "source": [
    "detector2 = Face_Detector()\n",
    "device = \"CPU\"\n",
    "cpu_extension_path = \"cpu_extension_avx2.dll\"\n",
    "detector2.load_net(model_xml,model_bin,\"CPU\",cpu_extension_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\Images\\query\\Mind Grenades - Fake Faces 2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "([(71, 69, 282, 328), (394, 67, 576, 324), (710, 74, 902, 322), (77, 436, 266, 673), (373, 436, 566, 689), (707, 432, 899, 682), (1003, 427, 1183, 691), (1670, 444, 1861, 685), (60, 795, 251, 1027), (383, 788, 576, 1053), (714, 797, 910, 1022), (1670, 795, 1852, 1030), (1668, 76, 1846, 334), (1320, 786, 1529, 1044), (1025, 85, 1220, 323), (1344, 74, 1533, 340), (1341, 434, 1532, 692), (1029, 785, 1231, 1051)], 18)\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "res_NCS = detector.detectFaces(img)\n",
    "end=time.time()\n",
    "\n",
    "print(int((end-start)*1000))\n",
    "print(res_NCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detector2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-aa301e30e25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectFaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detector2' is not defined"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "res = detector2.detectFaces(img)\n",
    "end=time.time()\n",
    "\n",
    "print(int((end-start)*1000))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
