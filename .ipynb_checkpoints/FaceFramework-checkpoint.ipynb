{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from os import listdir\n",
    "import shutil\n",
    "from scipy.spatial.distance import cdist\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot as plt\n",
    "from FaceDetectionModule import Face_Detector\n",
    "from FaceGazeModule import GazeEstimator\n",
    "from FaceLandmarksModule import Landmark_Extractor\n",
    "from FaceEncoderModule import Face_Encoder\n",
    "from AgeGenderModule import AgeGenderDetector\n",
    "from FaceEmotionModule import Face_Emotion\n",
    "from PersonDetectionModule import Person_Detection\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"MYRIAD\"\n",
    "cpu_extension_path = \"cpu_extension_avx2.dll\"\n",
    "yaw_threshold = 15\n",
    "detection_thresh = 0.7\n",
    "src_dir = \"../images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Detector Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 300, 300]\n",
      "Output Shape: [1, 1, 200, 7]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-detection/FP16/face-detection-retail-0004.xml\"\n",
    "model_bin = r\"Models/openvino/face-detection/FP16/face-detection-retail-0004.bin\"\n",
    "\n",
    "plugin = Face_Detector.init_plugin(device)\n",
    "\n",
    "faceDetector = Face_Detector()\n",
    "\n",
    "faceDetector.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Person Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 320, 544]\n",
      "Output Shape: [1, 1, 200, 7]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models\\openvino\\person-detection\\FP16\\person-detection-retail-0013.xml\"\n",
    "model_bin = r\"Models\\openvino\\person-detection\\FP16\\person-detection-retail-0013.bin\"\n",
    "\n",
    "plugin = Person_Detection.init_plugin(device)\n",
    "\n",
    "detector = Person_Detection()\n",
    "\n",
    "detector.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\ComputerVision\\images\\00pedxing01-articleLarge.jpg\")\n",
    "\n",
    "res = detector.detect(img)\n",
    "\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Emotion Estimation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 64, 64]\n",
      "Output Shape: [1, 5, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.xml\"\n",
    "model_bin =r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.bin\"\n",
    "\n",
    "emotionEstimator = Face_Emotion()\n",
    "\n",
    "emotionEstimator.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml = r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.xml\"\n",
    "model_bin =r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.bin\"\n",
    "\n",
    "emotionEstimator = Face_Emotion()\n",
    "\n",
    "emotionEstimator.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pose Estimator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 60, 60]\n",
      "Output Shape: [1, 1]*3 for (Y,P,R)\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/gaze-estimation/FP16/head-pose-estimation-adas-0001.xml\"\n",
    "model_bin =r\"Models/openvino/gaze-estimation/FP16/head-pose-estimation-adas-0001.bin\"\n",
    "\n",
    "gazeEstimator = GazeEstimator()\n",
    "\n",
    "gazeEstimator.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Align Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 60, 60]\n",
      "Output Shape: [1, 70]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-landmarks/FP16/facial-landmarks-35-adas-0002.xml\"\n",
    "model_bin = r\"Models/openvino/face-landmarks/FP16/facial-landmarks-35-adas-0002.bin\"\n",
    "\n",
    "landmarkExtractor = Landmark_Extractor()\n",
    "\n",
    "landmarkExtractor.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 128, 128]\n",
      "Output Shape: [1, 256, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-identification/FP16/face-reidentification-retail-0095.xml\"\n",
    "model_bin = r\"Models/openvino/face-identification/FP16/face-reidentification-retail-0095.bin\"\n",
    "\n",
    "faceEncoder = Face_Encoder()\n",
    "\n",
    "faceEncoder.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Age Gender Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 62, 62]\n",
      "Output Shape Age: [1, 1, 1, 1]\n",
      "Output Shape Gender: [1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/age-gender/FP16/age-gender-recognition-retail-0013.xml\"\n",
    "model_bin = r\"Models/openvino/age-gender/FP16/age-gender-recognition-retail-0013.bin\"\n",
    "\n",
    "ageDetector = AgeGenderDetector()\n",
    "\n",
    "ageDetector.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Male', 32, 0.71728516)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\Images\\query\\wis.jpeg\")\n",
    "result = ageDetector.detectGenderAge(img)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognition:       \n",
    "    #def Train_Model(self,image_dir_path,allowed_extensions = [\".jpg\",\".jpeg\",\".png\",\".gif\",\".bmp\"]):\n",
    "     #   failed = []\n",
    "      #  images = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in allowed_extensions)]\n",
    "       # img_count = len(images)\n",
    "        #count =0\n",
    "        #if len(images)>0:\n",
    "         #   for imgname in images:\n",
    "          #      imgpath = image_dir_path + \"/\" + imgname\n",
    "           #     img = face_recognition.load_image_file(imgpath)\n",
    "            #    try:\n",
    "             #       count = count+1\n",
    "              #      print(\"Training on Image: \" + str(count) + \" out of \" + str(img_count))\n",
    "               #     face_encoding = face_recognition.face_encodings(img)[0]\n",
    "                #    np.save(imgpath[0:-4] ,face_encoding)\n",
    "                #except:\n",
    "                 #   failed.append(imgname)\n",
    "                  #  continue\n",
    "        #return failed\n",
    "    \n",
    "    #def generate_enc_data(self,imagePath,resize=True,resizeWidth = 64):\n",
    "      #  img = face_recognition.load_image_file(imagePath)\n",
    "       # print(img.shape)\n",
    "        #if resize:\n",
    "         #   img = self.resizeImage(img,resizeWidth)\n",
    "        #enc = face_recognition.face_encodings(img)\n",
    "        #face_encoding = []\n",
    "        #if len(enc)>0:\n",
    "         #   face_encoding = face_recognition.face_encodings(img)[0]\n",
    "        #return face_encoding\n",
    "    \n",
    "    def LoadImageEncoding(self,path):\n",
    "        arr = np.load(path)\n",
    "        return arr\n",
    "\n",
    "    def LoadAllImageEncondings(self,image_dir_path):\n",
    "        encondings_files = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in [\".npy\"])]\n",
    "        return {enc_file[0:-4] : self.LoadImageEncoding(image_dir_path + \"/\" + enc_file) for enc_file in encondings_files}\n",
    "    \n",
    "    def TrainEnc(self,encodings,dimensions=3,metric=\"minkowski\"):\n",
    "        df = pd.DataFrame(encodings).transpose()\n",
    "        labels = list(df.index)\n",
    "        model = KNeighborsClassifier(n_neighbors=dimensions,metric=metric)\n",
    "        model.fit(df,labels)\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        return model,df\n",
    "\n",
    "    def GetNearestFaces(self,img_target):\n",
    "        img_target = img_target.reshape(1,-1)\n",
    "        nn = self.model.kneighbors(img_target)\n",
    "        return nn[0][0],nn[1][0]\n",
    "    \n",
    "    def GetNearestFacesAsString(self,img_target):\n",
    "        img_target = img_target.reshape(1,-1)\n",
    "        nn = self.model.kneighbors(img_target)\n",
    "        names = []\n",
    "        a2 = nn[1][0]\n",
    "        distances = []\n",
    "        for ix in range(0,len(a2)):\n",
    "            element = self.df.iloc[[a2[ix]]]\n",
    "            names.append(element.index[0])\n",
    "            distances.append(distance.euclidean(element.values[0],img_target))\n",
    "        return  names,a2,distances\n",
    "    \n",
    "    def SaveModel(self,path):\n",
    "        if self.model == None:\n",
    "            raise Exception(\"No models to save.\")\n",
    "            return\n",
    "        joblib.dump(self.model, path) \n",
    "        print(\"Saved Model: {}\".format(path))\n",
    "        \n",
    "    def LoadModel(self,path):\n",
    "        self.model = joblib.load(path)\n",
    "        print(\"Loaded Model: {}\".format(path))\n",
    "        \n",
    "    def SaveEncodingsDataframe(self,path):\n",
    "        #if self.df == None:\n",
    "         #   raise Exception(\"No data frames to save.\")\n",
    "          #  return\n",
    "        self.df.to_pickle(path)\n",
    "        print(\"Saved Dataset: {}\".format(path))\n",
    "        \n",
    "    def LoadEncodingsDataframe(self,path):\n",
    "        self.df = pd.read_pickle(path)\n",
    "        print(\"Loaded Dataset: {}\".format(path))\n",
    "        \n",
    "    def resizeImage(self,image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "        dim = None\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        if width is None and height is None:\n",
    "            return image\n",
    "\n",
    "        if width is None:\n",
    "            r = height / float(h)\n",
    "            dim = (int(w * r), height)\n",
    "\n",
    "        else:\n",
    "            r = width / float(w)\n",
    "            dim = (width, int(h * r))\n",
    "\n",
    "        resized = cv2.resize(image, dim, interpolation = inter)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(image_dir_path,yaw_threshold = 30,detection_thresh = 0.7,visualize = False,allowed_extensions = [\".jpg\",\".jpeg\",\".png\",\".gif\",\".bmp\"]):\n",
    "    failed = []\n",
    "    images = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in allowed_extensions)]\n",
    "    img_count = len(images)\n",
    "    count =0\n",
    "    if len(images)>0:\n",
    "        for imgname in images:\n",
    "            print(\"============================================================\")\n",
    "            imgpath = image_dir_path + \"/\" + imgname\n",
    "            img = cv2.imread(imgpath)\n",
    "            try:\n",
    "                count = count+1\n",
    "                print(\"Training on Image: \" + str(count) + \" out of \" + str(img_count))\n",
    "                faces,_ = faceDetector.detectFaces(img,detection_thresh)\n",
    "                if faces is None or len(faces) == 0:\n",
    "                    print(\"Skipping image, no faces detected.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                elif len(faces)>1:\n",
    "                    print(\"Skipping image, multiple faces detected.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                (x_min, y_min, x_max, y_max) = faces[0]\n",
    "                img = img[y_min:y_max,x_min:x_max]  \n",
    "                img = landmarkExtractor.prepare_face(img,visualize,False)\n",
    "                y,_,_ = gazeEstimator.detectFaces(img)\n",
    "                print(\"Yaw = \" + str(y))\n",
    "                if y>yaw_threshold or y< -yaw_threshold:\n",
    "                    print(\"Skipping image, face Yaw exceeded provided threshold.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                #if visualize:\n",
    "                 #   plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "                  #  plt.show()\n",
    "                face_encoding = faceEncoder.encode_face(img)\n",
    "                np.save(imgpath[0:-4] ,face_encoding)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                failed.append(imgname)\n",
    "                continue\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match a query image to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMatchedFace(img,rec,match_thresh = 11,visualize=False,yaw_threshold = 30,detection_thresh = 0.7,detectAgeGender = False):\n",
    "    faces,_ = faceDetector.detectFaces(img,detection_thresh)\n",
    "    ages = []\n",
    "    genders = []\n",
    "    if faces is None or len(faces) == 0:\n",
    "        return -1,None #No faces detected\n",
    "    detected = []\n",
    "    encs = []\n",
    "    coords = []\n",
    "    for (x_min, y_min, x_max, y_max) in faces:\n",
    "        crop = img[y_min:y_max,x_min:x_max]  \n",
    "        crop = landmarkExtractor.prepare_face(crop,visualize,False)\n",
    "        y,_,_ = gazeEstimator.detectFaces(img)\n",
    "        if y<yaw_threshold and y>-yaw_threshold:\n",
    "            face_encoding = faceEncoder.encode_face(crop)\n",
    "            encs.append(face_encoding)\n",
    "            coords.append((x_min, y_min, x_max, y_max))\n",
    "            if detectAgeGender:\n",
    "                age,gender = ageDetector.detectGenderAge(crop)\n",
    "                ages.append(age)\n",
    "                genders.append(gender)\n",
    "    if(len(encs) == 0):\n",
    "        return -2,None #Faces detected but all of them unfit for comparisons\n",
    "    start = time.time()\n",
    "    #dists = cdist(encs,df,\"cosine\")\n",
    "    for i,enc in enumerate(encs):       \n",
    "        names,indices,distances = rec.GetNearestFacesAsString(enc)\n",
    "        if(distances[0]>match_thresh):\n",
    "            if detectAgeGender:\n",
    "                detected.append(([\"N/A\"],[],distances,coords[i],genders[i],ages[i]))\n",
    "            else:\n",
    "                detected.append(([\"N/A\"],[],distances,coords[i],\"\",-1))\n",
    "            continue\n",
    "        if detectAgeGender:\n",
    "            detected.append((names,indices,distances,coords[i],genders[i],ages[i]))\n",
    "        else:\n",
    "            detected.append((names,indices,distances,coords[i],\"\",-1))\n",
    "    end = time.time()\n",
    "    print(\"Distance measured in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "    return 1,detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImageEncoding(path):\n",
    "    arr = np.load(path)\n",
    "    return arr\n",
    "\n",
    "def LoadAllImageEncondings(image_dir_path):\n",
    "    encondings_files = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in [\".npy\"])]\n",
    "    return {enc_file[0:-4] : LoadImageEncoding(image_dir_path + \"/\" + enc_file) for enc_file in encondings_files}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Encodings, Train and save the KNN model and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = FaceRecognition()\n",
    "encs = rec.LoadAllImageEncondings(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model: Models/encModel.pkl\n",
      "Saved Dataset: Models/encDataFrame.pkl\n"
     ]
    }
   ],
   "source": [
    "model,df = rec.TrainEnc(encs,3,metric=\"cosine\")\n",
    "rec.SaveModel(\"Models/encModel.pkl\")\n",
    "rec.SaveEncodingsDataframe(\"Models/encDataFrame.pkl\")\n",
    "#del encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load KNN Model For Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model: Models/encModel.pkl\n",
      "Loaded Dataset: Models/encDataFrame.pkl\n"
     ]
    }
   ],
   "source": [
    "rec = FaceRecognition()\n",
    "rec.LoadModel(\"Models/encModel.pkl\")\n",
    "rec.LoadEncodingsDataframe(\"Models/encDataFrame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance measured in 60 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\ADNEC- VW 3\\Notebooks\\Images\\query\\WhatsApp Video 2019-01-20 at 2.59.03 PM 424.jpg\")\n",
    "\n",
    "img = identify_image(img,rec = rec)\n",
    "\n",
    "cv2.imshow(\"Output\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_image(img,rec,detectAgeGender = True ,match_thresh = 13.55):\n",
    "    times = []\n",
    "\n",
    "    start = time.time()\n",
    "    srarus,detected = FindMatchedFace(img,rec,match_thresh=match_thresh,detectAgeGender=detectAgeGender)\n",
    "    end = time.time()\n",
    "\n",
    "    if(status == 1):\n",
    "        for (names,indices,distances,(x_min, y_min, x_max, y_max),age,gender) in detected:\n",
    "            name = names[0]\n",
    "            cv2.rectangle(img,(x_min,y_min),(x_max,y_max),(255,255,0),2)\n",
    "            cv2.putText(img,\"{},{} years,{}\".format(name,gender,age),(x_min,y_min-10)\n",
    "                            ,cv2.FONT_HERSHEY_COMPLEX,0.5,(255,255,255))\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def identify_images(input_dir):\n",
    "    times = []\n",
    "    for img_path in glob.glob(input_dir):\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        start = time.time()\n",
    "        status,detected = FindMatchedFace(img,df,match_thresh=0.65,visualize=False,yaw_threshold = 20\n",
    "                                          ,detectAgeGender=True)\n",
    "        end = time.time()\n",
    "        times.append(end-start)\n",
    "        print(\"Executed in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "        print(detected)\n",
    "\n",
    "        if(status == 1):\n",
    "            for (name,(x_min, y_min, x_max, y_max),age,gender) in detected:\n",
    "                cv2.rectangle(img,(x_min,y_min),(x_max,y_max),(255,255,0),2)\n",
    "                cv2.putText(img,\"{},{} years,{}\".format(name,gender,age),(x_min-5,y_min-10)\n",
    "                            ,cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255))\n",
    "                if name == \"Unknown\":\n",
    "                    cv2.imshow(name,cv2.imread(src_dir + \"/Unknown.png\"))\n",
    "                else:\n",
    "                    cv2.imshow(name,cv2.imread(src_dir + \"/\" +name+\".png\"))\n",
    "\n",
    "        cv2.imshow(\"Output\",img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    print(\"All Images Executed in average {} milliseconds.\".format(int(np.mean(times)*1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_images(\"../images/query/*.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance measured in 56 milliseconds.\n",
      "Executed in 115 milliseconds.\n",
      "Distance measured in 127 milliseconds.\n",
      "Executed in 227 milliseconds.\n",
      "Distance measured in 64 milliseconds.\n",
      "Executed in 124 milliseconds.\n",
      "Distance measured in 122 milliseconds.\n",
      "Executed in 220 milliseconds.\n",
      "Distance measured in 182 milliseconds.\n",
      "Executed in 319 milliseconds.\n",
      "Distance measured in 178 milliseconds.\n",
      "Executed in 317 milliseconds.\n",
      "Distance measured in 126 milliseconds.\n",
      "Executed in 225 milliseconds.\n",
      "Distance measured in 65 milliseconds.\n",
      "Executed in 123 milliseconds.\n",
      "Distance measured in 181 milliseconds.\n",
      "Executed in 317 milliseconds.\n",
      "Distance measured in 293 milliseconds.\n",
      "Executed in 507 milliseconds.\n",
      "Distance measured in 60 milliseconds.\n",
      "Executed in 119 milliseconds.\n",
      "Distance measured in 90 milliseconds.\n",
      "Executed in 148 milliseconds.\n",
      "Distance measured in 61 milliseconds.\n",
      "Executed in 125 milliseconds.\n",
      "Distance measured in 119 milliseconds.\n",
      "Executed in 221 milliseconds.\n",
      "Distance measured in 1075 milliseconds.\n",
      "Executed in 1795 milliseconds.\n",
      "Distance measured in 58 milliseconds.\n",
      "Executed in 119 milliseconds.\n",
      "Distance measured in 119 milliseconds.\n",
      "Executed in 216 milliseconds.\n",
      "Distance measured in 60 milliseconds.\n",
      "Executed in 119 milliseconds.\n",
      "Distance measured in 61 milliseconds.\n",
      "Executed in 121 milliseconds.\n",
      "Distance measured in 61 milliseconds.\n",
      "Executed in 120 milliseconds.\n",
      "Distance measured in 59 milliseconds.\n",
      "Executed in 127 milliseconds.\n",
      "Distance measured in 63 milliseconds.\n",
      "Executed in 123 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "for img_path in glob.glob(\"../images/query/*.*\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    start = time.time()\n",
    "    img = identify_image(img,rec,match_thresh=15)\n",
    "    end = time.time()\n",
    "    print(\"Executed in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "    cv2.imshow(\"output\",img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_Model(src_dir,visualize=False,yaw_threshold=yaw_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_dir(num_files,src_path,target_path,filter=\"*.png\"):\n",
    "    src_path = src_path + \"/\" + filter\n",
    "    image_paths = glob.glob(src_path)\n",
    "    indices = np.random.randint(0,len(image_paths)-1,size=(num_files))\n",
    "    for i in indices:\n",
    "        img_path = image_paths[i]\n",
    "        shutil.copy(img_path,target_path + \"/\" + img_path.split(\"\\\\\")[-1])\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_from_dir(1000,r\"E:\\DataScience_old\\Notebooks\\Datasets\\Celebs\\img_align_celeba_png.7z\\img_align_celeba_png\"\n",
    " #              ,r\"D:\\DataScience\\Notebooks\\ComputerVision\\FaceRecognitionFramework\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VideoStream(src=0,usePiCamera=True)\n",
    "\n",
    "vs.start()\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    if frame is None or frame.shape[0]==0:\n",
    "        continue\n",
    "    #frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    try:\n",
    "        frame = identify_image(frame)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,800)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,600)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
