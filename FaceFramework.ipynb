{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Python modules.\n",
      "Imported Python modules.\n",
      "Imported Python modules.\n",
      "Imported Python modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pi/.virtualenvs/openvino/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from os import listdir\n",
    "import shutil\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot as plt\n",
    "from FaceDetectionModule import Face_Detector\n",
    "from FaceGazeModule import GazeEstimator\n",
    "from FaceLandmarksModule import Landmark_Extractor\n",
    "from FaceEncoderModule import Face_Encoder\n",
    "from AgeGenderModule import AgeGenderDetector\n",
    "from FaceEmotionModule import Face_Emotion\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "from CentroidBasedObjectDetector import CentroidTracker\n",
    "from collections import deque\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"MYRIAD\"\n",
    "cpu_extension_path = \"cpu_extension_avx2.dll\"\n",
    "yaw_threshold = 180\n",
    "detection_thresh = 0.7\n",
    "src_dir = r\"/home/pi/Notebooks/FaceRecognitionFramework/Face_DB\"\n",
    "gender_acceptance_thresh = 0.6\n",
    "min_face_dim = 64\n",
    "load_emotion_module = True\n",
    "cap_w = 800\n",
    "cap_h = 600\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (cap_h, cap_w)\n",
    "maxQueueLen = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Detector Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 300, 300]\n",
      "Output Shape: [1, 1, 200, 7]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-detection/FP16/face-detection-retail-0004.xml\"\n",
    "model_bin = r\"Models/openvino/face-detection/FP16/face-detection-retail-0004.bin\"\n",
    "\n",
    "plugin = Face_Detector.init_plugin(device,cpu_extension_path)\n",
    "\n",
    "faceDetector = Face_Detector()\n",
    "\n",
    "faceDetector.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pose Estimator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 60, 60]\n",
      "Output Shape: [1, 1]*3 for (Y,P,R)\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/gaze-estimation/FP16/head-pose-estimation-adas-0001.xml\"\n",
    "model_bin =r\"Models/openvino/gaze-estimation/FP16/head-pose-estimation-adas-0001.bin\"\n",
    "\n",
    "gazeEstimator = GazeEstimator()\n",
    "\n",
    "gazeEstimator.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Align Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 60, 60]\n",
      "Output Shape: [1, 70]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-landmarks/FP16/facial-landmarks-35-adas-0002.xml\"\n",
    "model_bin = r\"Models/openvino/face-landmarks/FP16/facial-landmarks-35-adas-0002.bin\"\n",
    "\n",
    "landmarkExtractor = Landmark_Extractor()\n",
    "\n",
    "landmarkExtractor.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Face Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 128, 128]\n",
      "Output Shape: [1, 256, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/face-identification/FP16/face-reidentification-retail-0095.xml\"\n",
    "model_bin = r\"Models/openvino/face-identification/FP16/face-reidentification-retail-0095.bin\"\n",
    "\n",
    "faceEncoder = Face_Encoder()\n",
    "\n",
    "faceEncoder.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Age Gender Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 62, 62]\n",
      "Output Shape Age: [1, 1, 1, 1]\n",
      "Output Shape Gender: [1, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_xml = r\"Models/openvino/age-gender/FP16/age-gender-recognition-retail-0013.xml\"\n",
    "model_bin = r\"Models/openvino/age-gender/FP16/age-gender-recognition-retail-0013.bin\"\n",
    "\n",
    "ageDetector = AgeGenderDetector()\n",
    "\n",
    "ageDetector.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Emotion Estimation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: All network layers are supported.\n",
      "Input Shape: [1, 3, 64, 64]\n",
      "Output Shape: [1, 5, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "if load_emotion_module:\n",
    "\n",
    "    model_xml = r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.xml\"\n",
    "    model_bin = r\"Models/openvino/emotion-estimation/FP16/emotions-recognition-retail-0003.bin\"\n",
    "\n",
    "    emotionEstimator = Face_Emotion()\n",
    "\n",
    "    emotionEstimator.load_net(model_xml,model_bin,plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognition:       \n",
    "    #def Train_Model(self,image_dir_path,allowed_extensions = [\".jpg\",\".jpeg\",\".png\",\".gif\",\".bmp\"]):\n",
    "     #   failed = []\n",
    "      #  images = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in allowed_extensions)]\n",
    "       # img_count = len(images)\n",
    "        #count =0\n",
    "        #if len(images)>0:\n",
    "         #   for imgname in images:\n",
    "          #      imgpath = image_dir_path + \"/\" + imgname\n",
    "           #     img = face_recognition.load_image_file(imgpath)\n",
    "            #    try:\n",
    "             #       count = count+1\n",
    "              #      print(\"Training on Image: \" + str(count) + \" out of \" + str(img_count))\n",
    "               #     face_encoding = face_recognition.face_encodings(img)[0]\n",
    "                #    np.save(imgpath[0:-4] ,face_encoding)\n",
    "                #except:\n",
    "                 #   failed.append(imgname)\n",
    "                  #  continue\n",
    "        #return failed\n",
    "    \n",
    "    #def generate_enc_data(self,imagePath,resize=True,resizeWidth = 64):\n",
    "      #  img = face_recognition.load_image_file(imagePath)\n",
    "       # print(img.shape)\n",
    "        #if resize:\n",
    "         #   img = self.resizeImage(img,resizeWidth)\n",
    "        #enc = face_recognition.face_encodings(img)\n",
    "        #face_encoding = []\n",
    "        #if len(enc)>0:\n",
    "         #   face_encoding = face_recognition.face_encodings(img)[0]\n",
    "        #return face_encoding\n",
    "    def __init__(self):\n",
    "        from sklearn.externals import joblib\n",
    "    \n",
    "    def LoadImageEncoding(self,path):\n",
    "        arr = np.load(path)\n",
    "        return arr\n",
    "\n",
    "    def LoadAllImageEncondings(self,image_dir_path):\n",
    "        encondings_files = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in [\".npy\"])]\n",
    "        return {enc_file[0:-4] : self.LoadImageEncoding(image_dir_path + \"/\" + enc_file) for enc_file in encondings_files}\n",
    "    \n",
    "    def TrainEnc(self,encodings,dimensions=3,metric=\"minkowski\"):\n",
    "        df = pd.DataFrame(encodings).transpose()\n",
    "        labels = list(df.index)\n",
    "        model = KNeighborsClassifier(n_neighbors=dimensions,metric=metric)\n",
    "        model.fit(df,labels)\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        return model,df\n",
    "\n",
    "    def GetNearestFaces(self,img_target):\n",
    "        img_target = img_target.reshape(1,-1)\n",
    "        nn = self.model.kneighbors(img_target)\n",
    "        return nn[0][0],nn[1][0]\n",
    "    \n",
    "    def GetNearestFacesAsString(self,img_target):\n",
    "        img_target = img_target.reshape(1,-1)\n",
    "        nn = self.model.kneighbors(img_target)\n",
    "        names = []\n",
    "        a2 = nn[1][0]\n",
    "        distances = []\n",
    "        for ix in range(0,len(a2)):\n",
    "            element = self.df.iloc[[a2[ix]]]\n",
    "            names.append(element.index[0])\n",
    "            distances.append(distance.euclidean(element.values[0],img_target))\n",
    "        return  names,a2,distances\n",
    "    \n",
    "    def SaveModel(self,path):\n",
    "        if self.model == None:\n",
    "            raise Exception(\"No models to save.\")\n",
    "            return\n",
    "        joblib.dump(self.model, path) \n",
    "        print(\"Saved Model: {}\".format(path))\n",
    "        \n",
    "    def LoadModel(self,path):\n",
    "        self.model = joblib.load(path)\n",
    "        print(\"Loaded Model: {}\".format(path))\n",
    "        \n",
    "    def SaveEncodingsDataframe(self,path):\n",
    "        #if self.df == None:\n",
    "         #   raise Exception(\"No data frames to save.\")\n",
    "          #  return\n",
    "        self.df.to_pickle(path)\n",
    "        print(\"Saved Dataset: {}\".format(path))\n",
    "        \n",
    "    def LoadEncodingsDataframe(self,path):\n",
    "        self.df = pd.read_pickle(path)\n",
    "        print(\"Loaded Dataset: {}\".format(path))\n",
    "        \n",
    "    def resizeImage(self,image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "        dim = None\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        if width is None and height is None:\n",
    "            return image\n",
    "\n",
    "        if width is None:\n",
    "            r = height / float(h)\n",
    "            dim = (int(w * r), height)\n",
    "\n",
    "        else:\n",
    "            r = width / float(w)\n",
    "            dim = (width, int(h * r))\n",
    "\n",
    "        resized = cv2.resize(image, dim, interpolation = inter)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(image_dir_path,yaw_threshold = 30,detection_thresh = 0.7,visualize = False,allowed_extensions = [\".jpg\",\".jpeg\",\".png\",\".gif\",\".bmp\"]):\n",
    "    failed = []\n",
    "    images = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in allowed_extensions)]\n",
    "    img_count = len(images)\n",
    "    count =0\n",
    "    if len(images)>0:\n",
    "        for imgname in images:\n",
    "            print(\"============================================================\")\n",
    "            imgpath = image_dir_path + \"/\" + imgname\n",
    "            img = cv2.imread(imgpath)\n",
    "            try:\n",
    "                count = count+1\n",
    "                print(\"Training on Image: \" + str(count) + \" out of \" + str(img_count))\n",
    "                faces,_ = faceDetector.detectFaces(img,detection_thresh)\n",
    "                if faces is None or len(faces) == 0:\n",
    "                    print(\"Skipping image, no faces detected.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                elif len(faces)>1:\n",
    "                    print(\"Skipping image, multiple faces detected.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                (x_min, y_min, x_max, y_max) = faces[0]\n",
    "                img = img[y_min:y_max,x_min:x_max]  \n",
    "                img = landmarkExtractor.prepare_face(img,visualize,False)\n",
    "                y,_,_ = gazeEstimator.detectFaces(img)\n",
    "                print(\"Yaw = \" + str(y))\n",
    "                if y>yaw_threshold or y< -yaw_threshold:\n",
    "                    print(\"Skipping image, face Yaw exceeded provided threshold.\")\n",
    "                    failed.append(imgname)\n",
    "                    continue\n",
    "                #if visualize:\n",
    "                 #   plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "                  #  plt.show()\n",
    "                face_encoding = faceEncoder.encode_face(img)\n",
    "                np.save(imgpath[0:-4] ,face_encoding)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                failed.append(imgname)\n",
    "                continue\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_with_margin(img,x_min,y_min,x_max,y_max,margin_x_rate = 0.125 , margin_y_rate = 0.125):\n",
    "    margin_x = int((x_max-x_min) * margin_x_rate)\n",
    "    margin_y = int((y_max-y_min) * margin_y_rate)\n",
    "    img = img[max(y_min-margin_y,0):min(y_max+margin_y,img.shape[0]),max(x_min-margin_x-0,0):min(x_max+margin_x,img.shape[1])]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match a query image to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_Genders = {}\n",
    "rolling_ages = {}\n",
    "rolling_emotions = {}\n",
    "\n",
    "def FindMatchedFace(img,rec,match_thresh = 11,visualize=False,yaw_threshold = 30,detection_thresh = 0.7,detectAgeGender = False,verbose=True,cropimage=True):\n",
    "    rects = []\n",
    "    faces,_ = faceDetector.detectFaces(img,detection_thresh)\n",
    "    ages = []\n",
    "    genders = []\n",
    "    emotions = []\n",
    "    if faces is None or len(faces) == 0:\n",
    "        return -1,None #No faces detected\n",
    "    detected = []\n",
    "    encs = []\n",
    "    coords = []\n",
    "    for (x_min, y_min, x_max, y_max) in faces:\n",
    "        rects.append((x_min,y_min,x_max,y_max))\n",
    "    objects = ct.update(rects)\n",
    "    #print(objects.keys())\n",
    "    for i,(x_min, y_min, x_max, y_max) in enumerate(faces):\n",
    "        rolling_ix = list(objects.keys())[i]\n",
    "        \n",
    "        crop = img[y_min:y_max,x_min:x_max]  \n",
    "        if crop.shape[0]<min_face_dim or crop.shape[1]<min_face_dim:\n",
    "            continue\n",
    "        crop = landmarkExtractor.prepare_face(crop,visualize,False)\n",
    "        y,_,_ = gazeEstimator.detectFaces(img)\n",
    "        if y<yaw_threshold and y>-yaw_threshold:\n",
    "            face_encoding = faceEncoder.encode_face(crop)\n",
    "            encs.append(face_encoding)\n",
    "            coords.append((x_min, y_min, x_max, y_max))\n",
    "           # if load_emotion_module:\n",
    "            #    emotion = emotionEstimator.estimate_emotion(crop)\n",
    "             #   emotions.append(emotion)\n",
    "            #else:\n",
    "             #   emotions.append(\"\")\n",
    "            if detectAgeGender:\n",
    "                if cropimage:\n",
    "                    gender,age,prob = ageDetector.detectGenderAge(crop_with_margin(img,x_min,y_min,x_max,y_max))\n",
    "                    #gender,age,prob = ageDetector.detectGenderAge(crop)\n",
    "                    if prob<gender_acceptance_thresh:\n",
    "                        gender=\"Unsure\"\n",
    "\n",
    "                    emotion = emotionEstimator.estimate_emotion(crop)\n",
    "                    #print(rolling_ix)\n",
    "                    #print(rolling_Genders)\n",
    "                    ##print(rolling_Genders.keys())\n",
    "                    #print(rolling_Genders)\n",
    "                    #print(rolling_Genders.keys())\n",
    "                    if rolling_ix in rolling_Genders.keys():\n",
    "                        #print(\"Appending,,,\")\n",
    "                        rolling_Genders[rolling_ix].append(gender)\n",
    "                        rolling_ages[rolling_ix].append(age)\n",
    "                        rolling_emotions[rolling_ix].append(emotion)\n",
    "                    else:\n",
    "                        #print(\"Adding {}\".format(rolling_ix))\n",
    "                        Q_g = deque(maxlen = maxQueueLen)\n",
    "                        Q_g.append(gender)\n",
    "                        Q_em = deque(maxlen = maxQueueLen)\n",
    "                        Q_em.append(emotion)\n",
    "                        Q_age = deque(maxlen = maxQueueLen)\n",
    "                        Q_age.append(age)\n",
    "                        rolling_Genders[rolling_ix] = Q_g\n",
    "                        rolling_ages[rolling_ix] = Q_age\n",
    "                        rolling_emotions[rolling_ix] = Q_em\n",
    "                        \n",
    "                    Counter(rolling_ages[rolling_ix]).most_common(1)[0][0]\n",
    "                    ages.append(int(np.array(rolling_ages[rolling_ix]).mean(axis=0)))\n",
    "                    \n",
    "                    genders.append(Counter(rolling_Genders[rolling_ix]).most_common(1)[0][0])\n",
    "                    emotions.append(Counter(rolling_emotions[rolling_ix]).most_common(1)[0][0])\n",
    "                else:\n",
    "                    gender,age,prob = ageDetector.detectGenderAge(crop)\n",
    "                    ages.append(age)\n",
    "                    if prob<gender_acceptance_thresh:\n",
    "                        genders.append(\"Unsure\")\n",
    "                    else:\n",
    "                        genders.append(gender)\n",
    "    if(len(encs) == 0):\n",
    "        return -2,None #Faces detected but all of them unfit for comparisons\n",
    "    start = time.time()\n",
    "    #dists = cdist(encs,df,\"cosine\")\n",
    "    for i,enc in enumerate(encs):       \n",
    "        names,indices,distances = rec.GetNearestFacesAsString(enc)\n",
    "        if(distances[0]>match_thresh):\n",
    "            if detectAgeGender:\n",
    "                detected.append(([\"N/A\"],[],distances,coords[i],ages[i],genders[i],emotions[i]))\n",
    "            else:\n",
    "                detected.append(([\"N/A\"],[],distances,coords[i],\"\",-1,emotions[i]))\n",
    "            continue\n",
    "        if detectAgeGender:\n",
    "            detected.append((names,indices,distances,coords[i],ages[i],genders[i],emotions[i]))\n",
    "        else:\n",
    "            detected.append((names,indices,distances,coords[i],\"\",-1,emotions[i]))\n",
    "    end = time.time()\n",
    "    if verbose:\n",
    "        print(\"Distance measured in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "    return 1,detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImageEncoding(path):\n",
    "    arr = np.load(path)\n",
    "    return arr\n",
    "\n",
    "def LoadAllImageEncondings(image_dir_path):\n",
    "    encondings_files = [f for f in listdir(image_dir_path) if isfile(join(image_dir_path, f)) and (f.lower()[-4:] in [\".npy\"])]\n",
    "    return {enc_file[0:-4] : LoadImageEncoding(image_dir_path + \"/\" + enc_file) for enc_file in encondings_files}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Encodings, Train and save the KNN model and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_Model(src_dir,visualize=True,yaw_threshold=yaw_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec = FaceRecognition()\n",
    "#encs = rec.LoadAllImageEncondings(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(encs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model,df = rec.TrainEnc(encs,3,metric=\"cosine\")\n",
    "#rec.SaveModel(\"Models/encModel.pkl\")\n",
    "#rec.SaveEncodingsDataframe(\"Models/encDataFrame.pkl\")\n",
    "#del encs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load KNN Model For Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model: Models/encModel.pkl\n",
      "Loaded Dataset: Models/encDataFrame.pkl\n"
     ]
    }
   ],
   "source": [
    "rec = FaceRecognition()\n",
    "rec.LoadModel(\"Models/encModel.pkl\")\n",
    "rec.LoadEncodingsDataframe(\"Models/encDataFrame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_image(img,rec,detectAgeGender = True ,match_thresh = 13.55,verbose=True,cropimage=True):\n",
    "    times = []\n",
    "\n",
    "   # start = time.time()\n",
    "    status,detected = FindMatchedFace(img,rec,match_thresh=match_thresh,detectAgeGender=detectAgeGender,verbose=verbose,cropimage=cropimage)\n",
    "    #end = time.time()\n",
    "\n",
    "    if(status == 1):\n",
    "        for (names,indices,distances,(x_min, y_min, x_max, y_max),age,gender,emotion) in detected:\n",
    "            name = names[0]\n",
    "            cv2.rectangle(img,(x_min,y_min),(x_max,y_max),(255,255,0),2)\n",
    "            cv2.putText(img,\"{},{},{},{} years.\".format(name,gender,emotion,age),(x_min,y_min-10)\n",
    "                            ,cv2.FONT_HERSHEY_COMPLEX,0.5,(255,255,255))\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img = cv2.imread(r\"/home/pi/Notebooks/FaceRecognitionFramework/eval/ang.JPG\")\n",
    "start = time.time()\n",
    "img = identify_image(img,rec = rec,verbose=False,match_thresh = 16)\n",
    "end = time.time()\n",
    "\n",
    "print(int((end-start)*1000))\n",
    "\n",
    "cv2.imshow(\"Output\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "def identify_images(input_dir):\n",
    "    times = []\n",
    "    for img_path in glob.glob(input_dir):\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        start = time.time()\n",
    "        status,detected = FindMatchedFace(img,df,match_thresh=0.65,visualize=False,yaw_threshold = 20\n",
    "                                          ,detectAgeGender=True)\n",
    "        end = time.time()\n",
    "        times.append(end-start)\n",
    "        print(\"Executed in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "        print(detected)\n",
    "\n",
    "        if(status == 1):\n",
    "            for (name,(x_min, y_min, x_max, y_max),age,gender) in detected:\n",
    "                cv2.rectangle(img,(x_min,y_min),(x_max,y_max),(255,255,0),2)\n",
    "                cv2.putText(img,\"{},{} years,{}\".format(name,gender,age),(x_min-5,y_min-10)\n",
    "                            ,cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255))\n",
    "                if name == \"Unknown\":\n",
    "                    cv2.imshow(name,cv2.imread(src_dir + \"/Unknown.png\"))\n",
    "                else:\n",
    "                    cv2.imshow(name,cv2.imread(src_dir + \"/\" +name+\".png\"))\n",
    "\n",
    "        cv2.imshow(\"Output\",img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    print(\"All Images Executed in average {} milliseconds.\".format(int(np.mean(times)*1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify_images(\"../images/query/*.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for img_path in glob.glob(\"/home/pi/Notebooks/FaceRecognitionFramework/eval/*.*\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    start = time.time()\n",
    "    img = identify_image(img,rec,match_thresh=16,verbose=False)\n",
    "    end = time.time()\n",
    "    print(\"Executed in {} milliseconds.\".format(int((end-start)*1000)))\n",
    "    cv2.imshow(\"output\",img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def sample_from_dir(num_files,src_path,target_path,filter=\"*.png\"):\n",
    "    src_path = src_path + \"/\" + filter\n",
    "    image_paths = glob.glob(src_path)\n",
    "    indices = np.random.randint(0,len(image_paths)-1,size=(num_files))\n",
    "    for i in indices:\n",
    "        img_path = image_paths[i]\n",
    "        shutil.copy(img_path,target_path + \"/\" + img_path.split(\"\\\\\")[-1])\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_from_dir(1000,r\"E:\\DataScience_old\\Notebooks\\Datasets\\Celebs\\img_align_celeba_png.7z\\img_align_celeba_png\"\n",
    " #              ,r\"D:\\DataScience\\Notebooks\\ComputerVision\\FaceRecognitionFramework\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VideoStream(src=0,usePiCamera=True,resolution=(800,608))\n",
    "\n",
    "vs.start()\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    frame = vs.read()\n",
    "    if frame is None or frame.shape[0]==0:\n",
    "        continue\n",
    "    frame = cv2.cvtColor(cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY),cv2.COLOR_GRAY2BGR)\n",
    "    try:\n",
    "        frame = identify_image(frame,rec,match_thresh=19,verbose=False,cropimage=True)\n",
    "    except Exception as e :\n",
    "        print(str(e))\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,cap_w)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,cap_h)\n",
    "\n",
    "rolling_Genders = {}\n",
    "rolling_ages = {}\n",
    "rolling_emotions = {}\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        continue\n",
    "    # Display the resulting frame\n",
    "    #try:\n",
    "    frame = identify_image(frame,rec,match_thresh=16,verbose=False,cropimage=True)\n",
    "    #except Exception as e :\n",
    "        #print(str(e))\n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
